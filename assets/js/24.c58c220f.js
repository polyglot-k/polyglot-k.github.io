(window.webpackJsonp=window.webpackJsonp||[]).push([[24],{344:function(_,v,t){"use strict";t.r(v);var a=t(18),r=Object(a.a)({},(function(){var _=this,v=_._self._c;return v("ContentSlotsDistributor",{attrs:{"slot-key":_.$parent.slotKey}},[v("h1",{attrs:{id:"spring-webflux와-redis로-구축한-llm-스트리밍-파이프라인"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#spring-webflux와-redis로-구축한-llm-스트리밍-파이프라인"}},[_._v("#")]),_._v(" Spring WebFlux와 Redis로 구축한 LLM 스트리밍 파이프라인")]),_._v(" "),v("p",[_._v("AI 기반 대화형 서비스에서 핵심은 "),v("strong",[_._v("응답 속도")]),_._v("와 "),v("strong",[_._v("대화의 자연스러움")]),_._v("이다.\n“AI 면접관” 프로젝트를 시작할 때 세운 원칙은 단 하나였다.")]),_._v(" "),v("blockquote",[v("p",[v("strong",[_._v("“AI와의 대화는 사람과의 대화처럼 자연스러워야 한다.”")])])]),_._v(" "),v("p",[_._v("이 목표는 단순히 빠른 응답을 반환하는 것을 넘어,\nAI가 "),v("strong",[_._v("‘생각하며 말하는 듯한 실시간 스트리밍 경험’")]),_._v(" 을 구현해야 한다는 기술적 과제로 이어졌다.")]),_._v(" "),v("h2",{attrs:{id:"_1️⃣-아키텍처-설계-선택의-이유"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#_1️⃣-아키텍처-설계-선택의-이유"}},[_._v("#")]),_._v(" 1️⃣ 아키텍처 설계 — 선택의 이유")]),_._v(" "),v("h3",{attrs:{id:"webflux-비동기-i-o의-필연성"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#webflux-비동기-i-o의-필연성"}},[_._v("#")]),_._v(" WebFlux: 비동기 I/O의 필연성")]),_._v(" "),v("p",[_._v("LLM 응답은 특성상 수 초 이상 지연되는 경우가 많다.\n기존 Spring MVC의 스레드-퍼-리퀘스트 모델은 이런 장기 연결 요청에서 병목을 초래했다.")]),_._v(" "),v("p",[_._v("WebFlux는 소수의 논블로킹 스레드로 수천 개의 요청을 동시에 처리할 수 있다.\n즉, 요청마다 스레드를 점유하지 않아 "),v("strong",[_._v("높은 동시성 환경에서도 안정적인 처리량")]),_._v("을 확보할 수 있었다.")]),_._v(" "),v("p",[_._v("이 선택의 트레이드오프는 명확했다.\n비동기 처리를 도입함으로써 디버깅이 복잡해지고, 코드 가독성이 떨어질 수 있다.\n하지만, "),v("strong",[_._v("응답 대기 시간이 긴 LLM 요청의 특성상 WebFlux는 필연적인 선택")]),_._v("이었다.")]),_._v(" "),v("h3",{attrs:{id:"sse-단방향-스트리밍의-합리적-선택"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#sse-단방향-스트리밍의-합리적-선택"}},[_._v("#")]),_._v(" SSE: 단방향 스트리밍의 합리적 선택")]),_._v(" "),v("p",[_._v("AI 응답은 “서버 → 클라이언트”로만 흐른다.\n따라서 양방향 통신(WebSocket)보다는 "),v("strong",[_._v("HTTP 기반의 단방향 스트리밍 프로토콜인 SSE(Server-Sent Events)")]),_._v(" 가 훨씬 단순하고 효율적이었다.")]),_._v(" "),v("p",[_._v("SSE는 별도의 프로토콜 업그레이드 없이 브라우저 기본 API("),v("code",[_._v("EventSource")]),_._v(")로 바로 사용할 수 있으며,\n자동 재연결 기능을 지원해 안정성이 높다.")]),_._v(" "),v("p",[_._v("이 선택은 구조를 단순화했지만, 동시에 "),v("strong",[_._v("바이너리 데이터 전송 불가")]),_._v("라는 제약도 함께 가져왔다.\n텍스트 기반 응답만 다루는 LLM 서비스이기에, 이는 충분히 감수할 수 있는 범위였다.")]),_._v(" "),v("h3",{attrs:{id:"langchain4j-gemini-api와의-직접-연동"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#langchain4j-gemini-api와의-직접-연동"}},[_._v("#")]),_._v(" LangChain4j: Gemini API와의 직접 연동")]),_._v(" "),v("p",[_._v("당시 Spring AI는 Google Gemini의 스트리밍 API를 직접 지원하지 않았다.\n따라서 LLM 호출은 "),v("strong",[_._v("LangChain4j")]),_._v("를 사용하여 처리했다.")]),_._v(" "),v("p",[_._v("LangChain4j는 Gemini의 토큰 단위 스트리밍("),v("code",[_._v("TokenStream")]),_._v(")을 그대로 지원하며,\n향후 다른 LLM과의 체이닝 구조로 확장하기에도 유리했다.")]),_._v(" "),v("p",[_._v("이를 통해 AI가 한 번에 문장을 내뱉는 대신,\n"),v("strong",[_._v("“조금씩 말하면서 생각하는 듯한” 자연스러운 사용자 경험")]),_._v("을 구현할 수 있었다.")]),_._v(" "),v("h2",{attrs:{id:"_2️⃣-문제와-해결"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#_2️⃣-문제와-해결"}},[_._v("#")]),_._v(" 2️⃣ 문제와 해결")]),_._v(" "),v("h3",{attrs:{id:"스트림-끊김-하트비트로-연결-유지"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#스트림-끊김-하트비트로-연결-유지"}},[_._v("#")]),_._v(" 스트림 끊김 — 하트비트로 연결 유지")]),_._v(" "),v("p",[_._v("개발 초기, 약 30초 이상 응답이 지속되면 스트림이 조용히 끊어지는 문제가 발생했다.\n원인은 로드밸런서의 idle timeout이었다.")]),_._v(" "),v("p",[_._v("이 문제를 해결하기 위해, "),v("strong",[_._v("주기적으로 하트비트(:heartbeat) 이벤트를 전송")]),_._v("하도록 했다.\n하트비트는 클라이언트 화면에는 표시되지 않지만,\nTCP 연결을 유지시키는 역할을 수행했다.")]),_._v(" "),v("p",[_._v("이 단순한 개선으로 스트림 연결 안정성이 크게 높아졌고,\n서버나 네트워크 환경에 관계없이 지속적인 스트리밍이 가능해졌다.")]),_._v(" "),v("h3",{attrs:{id:"chunk-단위-저장으로-인한-db-부하-redis-임시-버퍼로-완화"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#chunk-단위-저장으로-인한-db-부하-redis-임시-버퍼로-완화"}},[_._v("#")]),_._v(" Chunk 단위 저장으로 인한 DB 부하 — Redis 임시 버퍼로 완화")]),_._v(" "),v("p",[_._v("LLM 응답은 수백 개의 토큰으로 나뉘어 전송된다.\n초기에는 각 토큰(Chunk)마다 DB에 저장하는 구조였는데,\n이로 인해 "),v("strong",[_._v("지속적인 I/O 부하")]),_._v("가 발생했다.")]),_._v(" "),v("p",[_._v("이를 해결하기 위해 Redis를 "),v("strong",[_._v("임시 버퍼")]),_._v("로 사용했다.")]),_._v(" "),v("p",[_._v("응답이 생성되는 동안 Redis에 Chunk 데이터를 순차적으로 누적하고,\n모든 응답이 완료된 시점에 한 번만 DB에 반영하도록 구조를 변경했다.")]),_._v(" "),v("p",[_._v("이 접근으로 DB I/O 부하를 대폭 줄였으며,\n특히 다수의 동시 스트리밍 요청에서도 안정적인 처리 성능을 확보할 수 있었다.")]),_._v(" "),v("h2",{attrs:{id:"_3️⃣-트레이드오프와-사이드이펙트"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#_3️⃣-트레이드오프와-사이드이펙트"}},[_._v("#")]),_._v(" 3️⃣ 트레이드오프와 사이드이펙트")]),_._v(" "),v("table",[v("thead",[v("tr",[v("th",[_._v("선택")]),_._v(" "),v("th",[_._v("장점")]),_._v(" "),v("th",[_._v("사이드이펙트")])])]),_._v(" "),v("tbody",[v("tr",[v("td",[_._v("WebFlux")]),_._v(" "),v("td",[_._v("높은 동시성 처리")]),_._v(" "),v("td",[_._v("디버깅 및 스택 추적 난이도 증가")])]),_._v(" "),v("tr",[v("td",[_._v("SSE")]),_._v(" "),v("td",[_._v("단순한 구조, 브라우저 호환성")]),_._v(" "),v("td",[_._v("바이너리 전송 불가")])]),_._v(" "),v("tr",[v("td",[_._v("Redis 버퍼")]),_._v(" "),v("td",[_._v("I/O 부하 감소")]),_._v(" "),v("td",[_._v("TTL 관리 실패 시 데이터 유실 가능성")])]),_._v(" "),v("tr",[v("td",[_._v("하트비트 유지")]),_._v(" "),v("td",[_._v("연결 안정성 향상")]),_._v(" "),v("td",[_._v("네트워크 트래픽 소폭 증가")])])])]),_._v(" "),v("p",[_._v("결국 모든 선택은 “대화의 자연스러움”이라는 목표를 달성하기 위한 트레이드오프였다.\nWebFlux의 복잡함, Redis TTL의 관리 리스크를 감수하더라도\n사용자가 "),v("strong",[_._v("‘생각하는 AI’와 대화하는 경험")]),_._v("을 제공하는 것이 더 큰 가치였다.")]),_._v(" "),v("h2",{attrs:{id:"_4️⃣-마무리하며"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#_4️⃣-마무리하며"}},[_._v("#")]),_._v(" 4️⃣ 마무리하며")]),_._v(" "),v("p",[_._v("이 파이프라인을 통해 단순한 “응답 반환”을 넘어,\nAI가 "),v("strong",[_._v("실시간으로 사고하고 말하는 듯한 대화 경험")]),_._v("을 구현할 수 있었다.")]),_._v(" "),v("p",[_._v("WebFlux는 비동기 처리의 복잡함을 동반하지만,\nReactive Stream의 철학을 이해하고 Redis를 적절히 조합하면\n"),v("strong",[_._v("확장성과 자연스러움")]),_._v("을 모두 잡을 수 있다.")])])}),[],!1,null,null,null);v.default=r.exports}}]);